{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision process\n",
    "\n",
    "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
    "\n",
    "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
    "\n",
    "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/blob/36a0b58261acde756abd55306fbe63df226bf62b/hw2/HW2.ipynb) _by Berkeley_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's define a simple MDP from this picture:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" alt=\"Diagram by Waldoalvarez via Wikimedia Commons, CC BY-SA 4.0\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# If you Colab, uncomment this please\n",
    "# !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use MDP just as any other gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it also has other methods that you'll need for Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "mdp.get_all_states = ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \",\n",
    "      mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Visualizing MDPs\n",
    "\n",
    "You can also visualize any MDP with the drawing fuction donated by [neer201](https://github.com/neer201).\n",
    "\n",
    "You have to install graphviz for system and for python. For ubuntu just run:\n",
    "\n",
    "1. `sudo apt-get install graphviz`\n",
    "2. `pip install graphviz`\n",
    "3. restart the notebook\n",
    "\n",
    "__Note:__ Installing graphviz on some OS (esp. Windows) may be tricky. However, you can ignore this part alltogether and use the standart vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Graphviz available: True\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from mdp import has_graphviz\n",
    "from IPython.display import display\n",
    "print(\"Graphviz available:\", has_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<graphviz.dot.Digraph at 0x7fd4c40f3f60>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n -->\n<!-- Title: MDP Pages: 1 -->\n<svg width=\"720pt\" height=\"227pt\"\n viewBox=\"0.00 0.00 720.00 226.97\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.704871 0.704871) rotate(0) translate(4 318)\">\n<title>MDP</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-318 1017.46,-318 1017.46,4 -4,4\"/>\n<!-- s0 -->\n<g id=\"node1\" class=\"node\"><title>s0</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"40\" cy=\"-116\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"40\" cy=\"-116\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-109.8\" font-family=\"Arial\" font-size=\"24.00\">s0</text>\n</g>\n<!-- s0&#45;a0 -->\n<g id=\"node2\" class=\"node\"><title>s0&#45;a0</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"192.577\" cy=\"-160\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"192.577\" y=\"-155\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n</g>\n<!-- s0&#45;&gt;s0&#45;a0 -->\n<g id=\"edge1\" class=\"edge\"><title>s0&#45;&gt;s0&#45;a0</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M79.0735,-124.577C99.3931,-129.435 124.733,-135.944 147,-143 150.338,-144.058 153.79,-145.23 157.227,-146.45\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"156.171,-149.791 166.764,-149.959 158.588,-143.221 156.171,-149.791\"/>\n</g>\n<!-- s0&#45;a1 -->\n<g id=\"node4\" class=\"node\"><title>s0&#45;a1</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"192.577\" cy=\"-233\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"192.577\" y=\"-228\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n</g>\n<!-- s0&#45;&gt;s0&#45;a1 -->\n<g id=\"edge4\" class=\"edge\"><title>s0&#45;&gt;s0&#45;a1</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M66.9211,-145.667C76.2366,-155.537 87.136,-166.237 98,-175 117.05,-190.365 140.454,-204.87 159.091,-215.532\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"157.386,-218.588 167.817,-220.443 160.82,-212.488 157.386,-218.588\"/>\n</g>\n<!-- s0&#45;a0&#45;&gt;s0 -->\n<g id=\"edge2\" class=\"edge\"><title>s0&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M165.016,-155.698C146.094,-152.302 120.188,-146.974 98,-140 94.1908,-138.803 90.2855,-137.447 86.3924,-136.004\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"87.3907,-132.637 76.802,-132.277 84.8555,-139.161 87.3907,-132.637\"/>\n<text text-anchor=\"middle\" x=\"122.5\" y=\"-158.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n</g>\n<!-- s2 -->\n<g id=\"node3\" class=\"node\"><title>s2</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"430.154\" cy=\"-183\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"430.154\" cy=\"-183\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"430.154\" y=\"-176.8\" font-family=\"Arial\" font-size=\"24.00\">s2</text>\n</g>\n<!-- s0&#45;a0&#45;&gt;s2 -->\n<g id=\"edge3\" class=\"edge\"><title>s0&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M220.143,-162.594C258.9,-166.378 331.61,-173.476 380.216,-178.222\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"379.908,-181.709 390.201,-179.197 380.588,-174.742 379.908,-181.709\"/>\n<text text-anchor=\"middle\" x=\"305.154\" y=\"-183.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.5</text>\n</g>\n<!-- s2&#45;a0 -->\n<g id=\"node8\" class=\"node\"><title>s2&#45;a0</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"662.731\" cy=\"-162\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"662.731\" y=\"-157\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n</g>\n<!-- s2&#45;&gt;s2&#45;a0 -->\n<g id=\"edge13\" class=\"edge\"><title>s2&#45;&gt;s2&#45;a0</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M470.104,-186.952C508.071,-189.784 567.218,-191.478 617.154,-181 620.829,-180.229 624.576,-179.15 628.26,-177.892\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"629.689,-181.093 637.769,-174.24 627.179,-174.559 629.689,-181.093\"/>\n</g>\n<!-- s2&#45;a1 -->\n<g id=\"node9\" class=\"node\"><title>s2&#45;a1</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"662.731\" cy=\"-80\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"662.731\" y=\"-75\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n</g>\n<!-- s2&#45;&gt;s2&#45;a1 -->\n<g id=\"edge16\" class=\"edge\"><title>s2&#45;&gt;s2&#45;a1</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M464.417,-161.955C472.086,-157.443 480.3,-152.873 488.154,-149 534.945,-125.928 591.475,-104.605 627.025,-91.9442\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"628.206,-95.2389 636.47,-88.6086 625.875,-88.6384 628.206,-95.2389\"/>\n</g>\n<!-- s0&#45;a1&#45;&gt;s2 -->\n<g id=\"edge5\" class=\"edge\"><title>s0&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M220.088,-229.012C255.125,-223.533 318.667,-212.879 372.154,-200 375.367,-199.226 378.667,-198.381 381.983,-197.493\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"383.207,-200.785 391.903,-194.733 381.331,-194.041 383.207,-200.785\"/>\n<text text-anchor=\"middle\" x=\"305.154\" y=\"-231.2\" font-family=\"Arial\" font-size=\"16.00\">p = 1</text>\n</g>\n<!-- s1 -->\n<g id=\"node5\" class=\"node\"><title>s1</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"824.309\" cy=\"-116\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"824.309\" cy=\"-116\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"824.309\" y=\"-109.8\" font-family=\"Arial\" font-size=\"24.00\">s1</text>\n</g>\n<!-- s1&#45;a0 -->\n<g id=\"node6\" class=\"node\"><title>s1&#45;a0</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"985.886\" cy=\"-92\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"985.886\" y=\"-87\" font-family=\"Arial\" font-size=\"20.00\">a0</text>\n</g>\n<!-- s1&#45;&gt;s1&#45;a0 -->\n<g id=\"edge6\" class=\"edge\"><title>s1&#45;&gt;s1&#45;a0</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M864.308,-112.227C886.639,-109.824 915.138,-106.344 940.309,-102 943,-101.535 945.774,-101.015 948.56,-100.462\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"949.505,-103.84 958.572,-98.3587 948.066,-96.9893 949.505,-103.84\"/>\n</g>\n<!-- s1&#45;a1 -->\n<g id=\"node7\" class=\"node\"><title>s1&#45;a1</title>\n<ellipse fill=\"lightpink\" stroke=\"lightpink\" cx=\"985.886\" cy=\"-174\" rx=\"27.6545\" ry=\"27.6545\"/>\n<text text-anchor=\"middle\" x=\"985.886\" y=\"-169\" font-family=\"Arial\" font-size=\"20.00\">a1</text>\n</g>\n<!-- s1&#45;&gt;s1&#45;a1 -->\n<g id=\"edge10\" class=\"edge\"><title>s1&#45;&gt;s1&#45;a1</title>\n<path fill=\"none\" stroke=\"red\" stroke-width=\"2\" d=\"M862.595,-127.85C885.226,-135.238 914.608,-145.181 940.309,-155 943.816,-156.34 947.454,-157.79 951.073,-159.271\"/>\n<polygon fill=\"red\" stroke=\"red\" stroke-width=\"2\" points=\"949.921,-162.583 960.497,-163.21 952.62,-156.124 949.921,-162.583\"/>\n</g>\n<!-- s1&#45;a0&#45;&gt;s0 -->\n<g id=\"edge7\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M966.263,-72.4892C938.379,-45.4834 882.871,-0 825.309,-0 191.577,-0 191.577,-0 191.577,-0 140.711,-0 96.5082,-42.0555 69.3862,-75.5217\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"66.6357,-73.3573 63.1979,-83.3788 72.1349,-77.6885 66.6357,-73.3573\"/>\n<text text-anchor=\"middle\" x=\"552.654\" y=\"-5.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.7 &#160;reward =5</text>\n</g>\n<!-- s1&#45;a0&#45;&gt;s2 -->\n<g id=\"edge9\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M970.152,-114.864C966.013,-121.814 961.716,-129.579 958.309,-137 947.928,-159.608 960.267,-175.15 940.309,-190 779.045,-309.99 687.054,-229.029 488.154,-200 484.884,-199.523 481.541,-198.899 478.195,-198.173\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"478.757,-194.708 468.212,-195.733 477.095,-201.508 478.757,-194.708\"/>\n<text text-anchor=\"middle\" x=\"737.309\" y=\"-259.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.2</text>\n</g>\n<!-- s1&#45;a0&#45;&gt;s1 -->\n<g id=\"edge8\" class=\"edge\"><title>s1&#45;a0&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M959.556,-83.5716C938.51,-77.8578 907.972,-72.5744 882.309,-80 876.619,-81.6464 870.951,-84.0488 865.518,-86.844\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"863.626,-83.8927 856.637,-91.8547 867.066,-89.9895 863.626,-83.8927\"/>\n<text text-anchor=\"middle\" x=\"911.309\" y=\"-85.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.1</text>\n</g>\n<!-- s1&#45;a1&#45;&gt;s2 -->\n<g id=\"edge11\" class=\"edge\"><title>s1&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M973.293,-198.859C965.835,-211.793 954.81,-226.446 940.309,-234 762.086,-326.847 675.051,-307.849 488.154,-234 479.811,-230.703 471.901,-225.594 464.777,-219.859\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"466.637,-216.838 456.797,-212.911 462.04,-222.117 466.637,-216.838\"/>\n<text text-anchor=\"middle\" x=\"737.309\" y=\"-301.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.05</text>\n</g>\n<!-- s1&#45;a1&#45;&gt;s1 -->\n<g id=\"edge12\" class=\"edge\"><title>s1&#45;a1&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M958.416,-170.931C937.242,-167.865 907.1,-162.112 882.309,-152 876.893,-149.791 871.42,-147.073 866.119,-144.129\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"867.794,-141.054 857.398,-139.01 864.25,-147.09 867.794,-141.054\"/>\n<text text-anchor=\"middle\" x=\"911.309\" y=\"-173.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.95</text>\n</g>\n<!-- s2&#45;a0&#45;&gt;s0 -->\n<g id=\"edge14\" class=\"edge\"><title>s2&#45;a0&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M635.999,-153.632C629.875,-151.882 623.326,-150.204 617.154,-149 443.606,-115.133 396.824,-130.338 220.154,-123 176.28,-121.178 126.356,-119.238 90.3736,-117.864\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"90.2707,-114.357 80.1447,-117.474 90.0041,-121.352 90.2707,-114.357\"/>\n<text text-anchor=\"middle\" x=\"305.154\" y=\"-131.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n</g>\n<!-- s2&#45;a0&#45;&gt;s2 -->\n<g id=\"edge15\" class=\"edge\"><title>s2&#45;a0&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M635.24,-158.041C600.919,-153.706 539.412,-148.649 488.154,-159 484.069,-159.825 479.913,-160.951 475.8,-162.269\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"474.585,-158.986 466.339,-165.638 476.934,-165.581 474.585,-158.986\"/>\n<text text-anchor=\"middle\" x=\"552.654\" y=\"-164.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.6</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s0 -->\n<g id=\"edge17\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s0</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M639.555,-64.6435C632.684,-60.7079 624.881,-57.0187 617.154,-55 454.135,-12.4077 405.633,-45.5527 238.154,-64 175.182,-70.9362 158.417,-70.9339 98,-90 93.9158,-91.2889 89.7396,-92.7963 85.5962,-94.4247\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"83.9351,-91.3249 76.0467,-98.3975 86.6239,-97.7879 83.9351,-91.3249\"/>\n<text text-anchor=\"middle\" x=\"305.154\" y=\"-69.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3 &#160;reward =&#45;1</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s2 -->\n<g id=\"edge19\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s2</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M636.31,-71.263C600.447,-60.607 534.239,-47.2134 488.154,-75 466.566,-88.0168 452.735,-112.241 444.113,-134.453\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"440.72,-133.545 440.619,-144.139 447.304,-135.92 440.72,-133.545\"/>\n<text text-anchor=\"middle\" x=\"552.654\" y=\"-80.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.4</text>\n</g>\n<!-- s2&#45;a1&#45;&gt;s1 -->\n<g id=\"edge18\" class=\"edge\"><title>s2&#45;a1&#45;&gt;s1</title>\n<path fill=\"none\" stroke=\"blue\" stroke-dasharray=\"5,2\" d=\"M689.725,-85.8641C712.772,-91.0633 747.027,-98.7911 775.078,-105.119\"/>\n<polygon fill=\"blue\" stroke=\"blue\" points=\"774.574,-108.594 785.099,-107.38 776.115,-101.765 774.574,-108.594\"/>\n<text text-anchor=\"middle\" x=\"737.309\" y=\"-108.2\" font-family=\"Arial\" font-size=\"16.00\">p = 0.3</text>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if has_graphviz:\n",
    "    from mdp import plot_graph, plot_graph_with_state_values, \\\n",
    "        plot_graph_optimal_strategy_and_state_values\n",
    "\n",
    "    display(plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Overwriting mdp_get_action_value.py\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%%writefile mdp_get_action_value.py\n",
    "\n",
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # state_values: dictionary of state:index\n",
    "    Q = 0\n",
    "    \n",
    "    for state_prime in mdp.get_next_states(state, action):\n",
    "        reward = mdp.get_reward(state, action, state_prime)\n",
    "        Q += mdp.get_transition_prob(state, action, state_prime) * (reward+gamma*state_values[state_prime])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from mdp_get_action_value import get_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'s0': 0, 's1': 1, 's2': 2}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "print(test_Vs)\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "    \n",
    "    Q_max = 0\n",
    "    \n",
    "    possible_actions = mdp.get_possible_actions(state) \n",
    "    for action_prime in possible_actions:\n",
    "        Q = get_action_value(mdp, state_values, state, action_prime, gamma)\n",
    "        if Q_max < Q:\n",
    "            Q_max = Q\n",
    "            print(Q_max)\n",
    "\n",
    "    return Q_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.9\n",
      "1.8\n",
      "1.08\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
    "assert test_Vs == test_Vs_copy, \"please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    new_state_values = <YOUR CODE HERE >\n",
    "\n",
    "    assert isinstance(new_state_values, dict)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 3.781) < 0.01\n",
    "assert abs(state_values['s1'] - 7.294) < 0.01\n",
    "assert abs(state_values['s2'] - 4.202) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    # <YOUR CODE HERE>\n",
    "\n",
    "    return < YOUR CODE >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    try:\n",
    "        display(plot_graph_optimal_strategy_and_state_values(mdp, state_values))\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Run the cell that starts with \\\"%%writefile mdp_get_action_value.py\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        new_state_values = <YOUR CODE HERE >\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = max(abs(new_state_values[s] - state_values[s])\n",
    "                   for s in mdp.get_all_states())\n",
    "\n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" %\n",
    "              (i, diff, new_state_values[mdp._initial_state]))\n",
    "\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize!\n",
    "\n",
    "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (0, 1)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None:\n",
    "                continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u*.3, -v*.3, color='m',\n",
    "                      head_width=0.1, head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to coursera\n",
    "\n",
    "If your submission doesn't finish in 30 seconds, set `verbose=True` and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from submit import submit_assigment\n",
    "submit_assigment(\n",
    "    get_action_value, \n",
    "    get_new_state_value, \n",
    "    get_optimal_action, \n",
    "    value_iteration, \n",
    "    <EMAIL>, \n",
    "    <TOKEN>,\n",
    "    verbose=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "kernelspec": {
   "name": "python2",
   "language": "python",
   "display_name": "Python 2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}